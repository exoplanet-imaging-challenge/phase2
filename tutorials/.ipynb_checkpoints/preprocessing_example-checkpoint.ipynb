{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example preprocessing of an IFS cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Author: *Valentin Christiaens*  \n",
    "> Last update: *2022/04/04*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**\n",
    "\n",
    "* [1. Loading and visualizing the data](#1.-Loading-and-visualizing-the-data)\n",
    "\n",
    "* [2. Preprocessing](#2.-Preprocessing)\n",
    "    - [2.1. Bad pixel correction](#2.1.-Bad-pixel-correction)\n",
    "    - [2.2. Bad frame removal](#2.2.-Bad-frame-removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to show how to further improve the quality of IFS data cubes provided in the phase 2 of the Exoplanet Data Challenge (as is from each instrument's official pipeline), before applying your favourite post-processing algorithms to detect the injected planets. \n",
    "\n",
    "Specifically, we show how to use the relevant functions of the `VIP` package in order to correct for remaining bad pixels and remove bad frames in the second SPHERE/IFS datacube used in the data challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import the packages needed in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hciplot import plot_frames, plot_cubes\n",
    "from matplotlib.pyplot import *\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from os.path import isfile\n",
    "import pandas as pd\n",
    "from vip_hci.fits import open_fits, write_fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 'dataset' folder of the `phase2` repository you can find a toy SPHERE/IFS coronagraphic cube acquired in pupil-stabilized mode on the source HIP39826 (a star with no reported directly imaged companion). The folder also contains the associated non-coronagraphic point spread function (PSF), wavelength vector and parallactic angles (the latter also including information on the airmass of the source).\n",
    "\n",
    "Let's now load the data. Note that more info on opening and visualizing fits files with VIP in general is available [in the first VIP tutorial](https://vip.readthedocs.io/en/latest/tutorials/01_quickstart.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace next box when zenodo link available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/Valentin/Documents/Postdoc/EIDC/Cubes/data_test/'\n",
    "datpath = path+'datasets/'\n",
    "\n",
    "cubename = datpath+'image_cube_sphere2.fits'\n",
    "angname = datpath+'parallactic_angles_sphere2.fits'\n",
    "\n",
    "cube = open_fits(cubename)\n",
    "derot_angles = open_fits(angname)[0]\n",
    "\n",
    "nch, nz, ny, nx = cube.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each IFS spectral cube consists of 39 monochromatic images spread in wavelengths between the Y and J bands ('YJ' mode) or Y and H bands ('YJH' mode). Here the IFS+ADI cube contains 65 such spectral cubes combined into a single master cube. The first column of the parallactic angle file actually contains the parallactic (derotation) angles, while the second columns contains the airmass at which each spectral cube was obtained.\n",
    "\n",
    "The **master spectral cube** is already centered. Let's inspect the first and last wavelengths, using `hciplot.plot_cubes` (feel free to set the backend to 'bokeh' to read pixel values interactively):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cubes(cube[0])#, backend='bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_cubes(cube[-5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspection of these two cubes shows the presence of residual bad pixels (e.g. in frames 7 and 30 of cube 0), and the presence of frames very different from the others (e.g. frame 61 in the last cube).\n",
    "\n",
    "In the next section, we will show how to better prepare the master cube by correcting bad pixels and trimming bad frames, in order to increase the performance of typical post-processing algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to the top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Bad pixel correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vip_hci.preproc import cube_fix_badpix_isolated, cube_fix_badpix_clump, cube_fix_badpix_with_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first identify static bad pixels - this is done with a sigma filtering algorithm applied to the mean frame (temporal dimension) at each wavelength:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cube_corr = cube.copy()\n",
    "bpm_mask_static = np.zeros([nch, ny, nx])\n",
    "for i in range(nch):\n",
    "    cube_corr[i], bpm_mask_static[i] = cube_fix_badpix_isolated(cube[i], sigma_clip=4, num_neig=9, size=9, \n",
    "                                                                frame_by_frame=False, mad=False, \n",
    "                                                                verbose=True, full_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now identify individual bad pixels in each frame (e.g. due to cosmic rays) with an iterative algorithm (suited to correct bad pixel clumps), and add them to the static bad pixel map (**warning: this may take some time depending on your machine** hence is by default deactivated - set `overwrite=True` to run the next cell anyway):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overwrite = False\n",
    "\n",
    "if not isfile(datpath+\"master_bad_pixel_map.fits\") or overwrite:\n",
    "    bpm_mask = np.zeros([nch, nz, ny, nx])\n",
    "    for i in range(nch):\n",
    "        cube_corr[i], bpm_mask_indiv = cube_fix_badpix_clump(cube_corr[i], fwhm=6, sig=4,\n",
    "                                                             mad=True, min_thr=(-0.5,2),\n",
    "                                                             verbose=True, full_output=True) \n",
    "        for z in range(nz):\n",
    "            bpm_mask[i,z] = bpm_mask_indiv[z]+bpm_mask_static[i]\n",
    "\n",
    "    bpm_mask[np.where(bpm_mask>1)]=1       \n",
    "    write_fits(datpath+\"master_bad_pixel_map.fits\", bpm_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now correct all identified bad pixels with a Gaussian kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpm_mask = open_fits(datpath+\"master_bad_pixel_map.fits\")\n",
    "for i in range(nch):\n",
    "    cube_corr[i] = cube_fix_badpix_with_kernel(cube[i], bpm_mask=bpm_mask[i], mode='gauss', fwhm=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare 2 frames before and after bad pixel correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "idx = 7\n",
    "plot_frames((cube[0,idx], cube_corr[0,idx], bpm_mask[0,idx]*cube[0,idx]), \n",
    "            vmin=0, vmax=float(np.amax(cube[0,idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -2\n",
    "plot_frames((cube[-1,idx], cube_corr[-1,idx], bpm_mask[-1,idx]*cube[-1,idx]), \n",
    "            vmin=0, vmax=float(np.amax(cube[-1,idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it may be possible to obtain better bad pixel corrections by tweaking the parameters of each routine used to identify them (such as `sigma_clip`, `size`, `sig` or `min_thr`)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to the top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Bad frames trim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now remove bad frames, based on the cross-correlation between each frame and the median of the ADI sequence (at each wavelength). We use the Structural Similarity (SSIM) index (REF), computed in an annular region beyond the coronagraphic mask. It is worth running a first pass of the algorithm using a percentile threshold, plot the values for each cube, and then  define an absolute threshold based on the mean SSIM values (over all wavelengths):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vip_hci.preproc import cube_detect_badfr_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_detect_badfr_correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssim = np.zeros([nch, nz])\n",
    "for i in range(nch):\n",
    "    good_idx, bad_idx, ssim[i] = cube_detect_badfr_correlation(cube_corr[i], \n",
    "                                                                frame_ref=np.median(cube_corr[i], axis=0), \n",
    "                                                                crop_size=61, dist='ssim', percentile=10, \n",
    "                                                                mode='annulus', inradius=8, width=18, \n",
    "                                                                plot=True, verbose=True, full_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that other distances can be used, such as the Pearson correlation coefficient. \n",
    "\n",
    "Although a plot was made for the ADI cube at each wavelength, let's visualize this better by plotting the measured SSIM values for a few different wavelengths, and the average over all channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,nz+1), ssim[0], 'bo', label='ch. 0', alpha=0.7)\n",
    "plt.plot(range(1,nz+1), ssim[20], 'yo', label='ch. 20', alpha=0.7)\n",
    "plt.plot(range(1,nz+1), ssim[20], 'ro', label='ch. 39', alpha=0.7)\n",
    "plt.plot(range(1,nz+1), np.mean(ssim,axis=0), 'k', label='mean over all ch.')\n",
    "plt.plot(range(1,nz+1), [np.percentile(np.mean(ssim,axis=0),10)]*nz, 'k:', alpha=0.5, label='10% threshold')\n",
    "plt.plot(range(1,nz+1), [np.percentile(np.mean(ssim,axis=0),15)]*nz, 'k--', alpha=0.6, label='15% threshold')\n",
    "plt.plot(range(1,nz+1), [np.percentile(np.mean(ssim,axis=0),20)]*nz, 'k-.', alpha=0.7, label='20% threshold')\n",
    "plt.xlabel(r\"Cube index\")\n",
    "plt.ylabel(r\"SSIM\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the trends are very similar at all wavelengths: the end of the sequence suffered from much worse conditions, as the dropping SSIM values testify. \n",
    "\n",
    "You can finally set a threshold in terms of percentile to remove bad frames depending on your post-processing algorithm (how sensitive it is to very different PSFs) and the regime in which the candidate you are looking after is located (speckle-dominated vs photon noise dominated).\n",
    "\n",
    "Here, let's consider for example removing the 15% worst frames. THe mean appears to follow closely the last channel, so we'll just get the good indices from the last spectral channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_thr = 15\n",
    "good_idx, bad_idx = cube_detect_badfr_correlation(cube_corr[-1], frame_ref=np.median(cube_corr[-1], axis=0), \n",
    "                                                  crop_size=61, dist='ssim', percentile=perc_thr, \n",
    "                                                  mode='annulus', inradius=8, width=18, \n",
    "                                                  plot=True, verbose=True, full_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save the master cube and associated parallactic angles after bad frame trim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_fits(datpath+'image_cube_sphere2_ready.fits', cube_corr[:,good_idx])\n",
    "write_fits(datpath+'parallactic_angles_sphere2_ready.fits', pa[:,good_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to the top](#Table-of-contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
